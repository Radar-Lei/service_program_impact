{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_core.documents import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 筛选跟每个service program 相关的posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 423023 documents (rows) from CSVs after filtering for Shenzhen metro related content\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "directory_path = '深圳地铁'\n",
    "all_docs = []\n",
    "\n",
    "# Define the Shenzhen metro station names\n",
    "sz_metro_stations = [\n",
    "    \"机场东\", \"机场\", \"宝安中心\", \"前海湾\", \"鲤鱼门\", \"大新\", \"桃园\", \"深大\", \"科苑\", \"白石洲\", \n",
    "    \"世界之窗\", \"华侨城\", \"侨城北\", \"香蜜湖\", \"车公庙\", \"竹子林\", \"招商银行大厦\", \"深康\", \"黄贝岭\", \n",
    "    \"黄贝\", \"新秀\", \"莲塘\", \"梧桐山南\", \"梧桐山\", \"盐田路\", \"沙头角\", \"海山\", \"罗湖\", \"国贸\", \n",
    "    \"老街\", \"大剧院\", \"科学馆\", \"华强路\", \"岗厦\", \"会展中心\", \"香蜜\", \"深南香蜜\", \"红树湾\", \"后海\", \n",
    "    \"南山\", \"科技园\", \"大学城\", \"桃源村\", \"龙珠\", \"龙华\", \"清湖\", \"碧海湾\", \"铁路公园\", \"西丽湖\"\n",
    "]\n",
    "\n",
    "# Define keywords for filtering\n",
    "keywords = [\"深圳\", \"深铁\"] + sz_metro_stations\n",
    "\n",
    "# Iterate through CSV files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Use pandas to read the CSV and preserve row information\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if \"微博正文\" column exists\n",
    "        if \"微博正文\" not in df.columns:\n",
    "            print(f\"Warning: '微博正文' column not found in {file_path}. Skipping file.\")\n",
    "            continue\n",
    "        \n",
    "        # Filter rows where \"微博正文\" contains any of the keywords\n",
    "        filtered_df = df[df[\"微博正文\"].apply(lambda text: \n",
    "            any(keyword in str(text) for keyword in keywords) if pd.notna(text) else False\n",
    "        )]\n",
    "        \n",
    "        # Convert each filtered row to a Document with appropriate metadata\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            # Convert row to string representation\n",
    "            content = row[\"微博正文\"]\n",
    "            # Create metadata with file source and row index\n",
    "            metadata = {\n",
    "                'source': file_path,\n",
    "                'row': index,  # Keep original index from source file\n",
    "                # You can add other metadata from your CSV if needed\n",
    "            }\n",
    "            doc = Document(page_content=content, metadata=metadata)\n",
    "            all_docs.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} documents (rows) from CSVs after filtering for Shenzhen metro related content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "texts_all = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma_rag/'\n",
    "posts_langchain_chroma = Chroma.from_documents(\n",
    "    documents=texts_all,\n",
    "    collection_name=\"SZ_posts_senti\",\n",
    "    embedding=hg_embeddings,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma_rag/'\n",
    "posts_langchain_chroma = Chroma(\n",
    "    collection_name=\"SZ_posts_senti\",\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=hg_embeddings,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"} # this make sure the similarity socre between [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description Data for Service Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"service_program_data/SPD_SZ_zh.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('senti_results_SZ/senti_cleaned_深圳 地铁 201901 1.0.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Create directory for output if it doesn't exist\n",
    "similarity_threshold = 0.35\n",
    "\n",
    "output_dir = f'similarity_threshold={similarity_threshold}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the service program CSV file\n",
    "service_program_df = pd.read_csv('service_program_data/SPD_SZ_zh.csv')\n",
    "\n",
    "# Create a cache for the source dataframes to avoid reloading them\n",
    "source_df_cache = {}\n",
    "\n",
    "# Iterate through each row of the CSV\n",
    "for index, row in service_program_df.iterrows():\n",
    "    # Concatenate the first three columns as service_dimension\n",
    "    service_dimension = ' '.join([str(row[col]) for col in service_program_df.columns[1:3] \n",
    "                            if pd.notna(row[col]) and str(row[col]) != 'nan'])\n",
    "    \n",
    "    # Search for similar documents in the vector database\n",
    "    docs = posts_langchain_chroma.similarity_search_with_relevance_scores(service_dimension, k=500, score_threshold=similarity_threshold)\n",
    "    # Create a dictionary to group matched rows by source file\n",
    "    matches_by_source = {}\n",
    "    \n",
    "    # Process each matched document\n",
    "    # each doc is a tuple\n",
    "    for doc in docs:\n",
    "        source = doc[0].metadata.get('source', None)\n",
    "        row_index = doc[0].metadata.get('row', None)\n",
    "        \n",
    "        if source and row_index is not None:\n",
    "            # Convert row to integer if it's a numeric string\n",
    "            try:\n",
    "                row_index = int(row_index)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "            if source not in matches_by_source:\n",
    "                matches_by_source[source] = []\n",
    "            \n",
    "            matches_by_source[source].append(row_index)\n",
    "    \n",
    "    # Create a list to store all matching rows from original sources\n",
    "    all_matched_rows = []\n",
    "    \n",
    "    # For each source file, get the matching rows\n",
    "    for source, row_indices in matches_by_source.items():\n",
    "        # Load the source file if not in cache\n",
    "        if source not in source_df_cache:\n",
    "            try:\n",
    "                source_df_cache[source] = pd.read_csv(source)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {source}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        source_df = source_df_cache[source]\n",
    "        \n",
    "        # Get rows from original source file\n",
    "        for row_idx in row_indices:\n",
    "            try:\n",
    "                if 0 <= row_idx < len(source_df):\n",
    "                    row_data = source_df.iloc[row_idx].to_dict()\n",
    "                    row_data['original_source'] = source\n",
    "                    row_data['original_row'] = row_idx\n",
    "                    all_matched_rows.append(row_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing row {row_idx} in {source}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_matched_rows:\n",
    "        matches_df = pd.DataFrame(all_matched_rows)\n",
    "        \n",
    "        # Create a filename for this service program\n",
    "        safe_filename = f\"service_program_{index}_matches.csv\"\n",
    "        \n",
    "        # Save to CSV\n",
    "        matches_df.to_csv(os.path.join(output_dir, safe_filename), index=False)\n",
    "    \n",
    "    # Print progress\n",
    "    if index % 10 == 0:\n",
    "        print(f\"Processed {index} rows\")\n",
    "\n",
    "print(f\"Completed! All match files saved to {output_dir} directory.\")\n",
    "\n",
    "# 创建摘要文件\n",
    "summary_file_path = os.path.join(output_dir, '0.matched_summary.txt')\n",
    "with open(summary_file_path, 'w', encoding='utf-8') as summary_file:\n",
    "    summary_file.write(f\"Created CSV files with matching posts:\\n\")\n",
    "    for i, filename in enumerate(sorted(os.listdir(output_dir))):\n",
    "        if filename == '0.matched_summary.txt':  # 跳过摘要文件本身\n",
    "            continue\n",
    "        try:\n",
    "            match_df = pd.read_csv(os.path.join(output_dir, filename))\n",
    "            summary_line = f\"- {filename}: {len(match_df)} matched original posts\\n\"\n",
    "            summary_file.write(summary_line)\n",
    "            print(summary_line, end='')\n",
    "        except:\n",
    "            summary_line = f\"- {filename}: Could not read file\\n\"\n",
    "            summary_file.write(summary_line)\n",
    "            print(summary_line, end='')\n",
    "\n",
    "print(f\"\\nSummary saved to {summary_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
