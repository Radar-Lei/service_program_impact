{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_core.documents import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 筛选跟每个service program 相关的posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 58738 documents (rows) from CSVs\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "directory_path = 'senti_results_SZ'\n",
    "all_docs = []\n",
    "\n",
    "# Iterate through CSV files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Use pandas to read the CSV and preserve row information\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert each row to a Document with appropriate metadata\n",
    "        for index, row in df.iterrows():\n",
    "            # Convert row to string representation\n",
    "            content = row.to_string()\n",
    "            # Create metadata with file source and row index\n",
    "            metadata = {\n",
    "                'source': file_path,\n",
    "                'row': index,\n",
    "                # You can add other metadata from your CSV if needed\n",
    "            }\n",
    "            doc = Document(page_content=content, metadata=metadata)\n",
    "            all_docs.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} documents (rows) from CSVs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "texts_all = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_directory = 'docs/chroma_rag/'\n",
    "# posts_langchain_chroma = Chroma.from_documents(\n",
    "#     documents=texts_all,\n",
    "#     collection_name=\"SZ_posts_senti\",\n",
    "#     embedding=hg_embeddings,\n",
    "#     persist_directory=persist_directory,\n",
    "#     collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma_rag/'\n",
    "posts_langchain_chroma = Chroma(\n",
    "    collection_name=\"SZ_posts_senti\",\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=hg_embeddings,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"} # this make sure the similarity socre between [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description Data for Service Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"service_program_data/SPD_SZ_zh.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load the CSV file\n",
    "service_program_df = pd.read_csv('service_program_data/SPD_SZ_zh.csv')\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each row of the CSV\n",
    "for index, row in service_program_df.iterrows():\n",
    "    # Concatenate the first three columns as service_dimension\n",
    "    service_dimension = ' '.join([str(row[col]) for col in service_program_df.columns[:3]])\n",
    "    \n",
    "    # Search for similar documents in the vector database\n",
    "    docs = posts_langchain_chroma.similarity_search(service_dimension, k=100)  # You can adjust k as needed\n",
    "    \n",
    "    # Store the results\n",
    "    result = {\n",
    "        'service_dimension': service_dimension,\n",
    "        'similar_documents': docs\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress (optional)\n",
    "    if index % 10 == 0:\n",
    "        print(f\"Processed {index} rows\")\n",
    "\n",
    "# Display some results (for example, the first one)\n",
    "if results:\n",
    "    for i in range(len(results)):\n",
    "      print(f\"Service dimension: {results[i]['service_dimension']}\")\n",
    "      print(\"\\nSimilar documents:\")\n",
    "      for j, doc in enumerate(results[i]['similar_documents']):\n",
    "          if j < 10:\n",
    "            print(f\"\\nDocument {j+1}:\")\n",
    "            print(f\"Content: {doc.page_content[200:]}...\")  # Display first 200 chars\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "            print(f\"Metadata: {doc.metadata.get('source', 'N/A')}, Row: {doc.metadata.get('row', 'N/A')}\")\n",
    "      print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('senti_results_SZ/senti_cleaned_深圳 地铁 201901 1.0.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n",
      "Processed 10 rows\n",
      "Processed 20 rows\n",
      "Completed! All match files saved to service_program_matches_SZ directory.\n",
      "Created CSV files with matching posts:\n",
      "- service_program_0_matches.csv: 4 matched original posts\n",
      "- service_program_10_matches.csv: 300 matched original posts\n",
      "- service_program_11_matches.csv: 300 matched original posts\n",
      "- service_program_12_matches.csv: 180 matched original posts\n",
      "- service_program_13_matches.csv: 300 matched original posts\n",
      "- service_program_14_matches.csv: 300 matched original posts\n",
      "- service_program_15_matches.csv: 300 matched original posts\n",
      "- service_program_16_matches.csv: 300 matched original posts\n",
      "- service_program_17_matches.csv: 5 matched original posts\n",
      "- service_program_18_matches.csv: 6 matched original posts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# Create directory for output if it doesn't exist\n",
    "output_dir = 'service_program_matches_SZ'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the service program CSV file\n",
    "service_program_df = pd.read_csv('service_program_data/SPD_SZ_zh.csv')\n",
    "\n",
    "# Create a cache for the source dataframes to avoid reloading them\n",
    "source_df_cache = {}\n",
    "\n",
    "# Iterate through each row of the CSV\n",
    "for index, row in service_program_df.iterrows():\n",
    "    # Concatenate the first three columns as service_dimension\n",
    "    service_dimension = ' '.join([str(row[col]) for col in service_program_df.columns[:3]])\n",
    "    \n",
    "    # Search for similar documents in the vector database\n",
    "    docs = posts_langchain_chroma.similarity_search_with_relevance_scores(service_dimension, k=300, score_threshold=0.2)    \n",
    "    # Create a dictionary to group matched rows by source file\n",
    "    matches_by_source = {}\n",
    "    \n",
    "    # Process each matched document\n",
    "    # each doc is a tuple\n",
    "    for doc in docs:\n",
    "        source = doc[0].metadata.get('source', None)\n",
    "        row_index = doc[0].metadata.get('row', None)\n",
    "        \n",
    "        if source and row_index is not None:\n",
    "            # Convert row to integer if it's a numeric string\n",
    "            try:\n",
    "                row_index = int(row_index)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "            if source not in matches_by_source:\n",
    "                matches_by_source[source] = []\n",
    "            \n",
    "            matches_by_source[source].append(row_index)\n",
    "    \n",
    "    # Create a list to store all matching rows from original sources\n",
    "    all_matched_rows = []\n",
    "    \n",
    "    # For each source file, get the matching rows\n",
    "    for source, row_indices in matches_by_source.items():\n",
    "        # Load the source file if not in cache\n",
    "        if source not in source_df_cache:\n",
    "            try:\n",
    "                source_df_cache[source] = pd.read_csv(source)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {source}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        source_df = source_df_cache[source]\n",
    "        \n",
    "        # Get rows from original source file\n",
    "        for row_idx in row_indices:\n",
    "            try:\n",
    "                if 0 <= row_idx < len(source_df):\n",
    "                    row_data = source_df.iloc[row_idx].to_dict()\n",
    "                    row_data['original_source'] = source\n",
    "                    row_data['original_row'] = row_idx\n",
    "                    all_matched_rows.append(row_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing row {row_idx} in {source}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_matched_rows:\n",
    "        matches_df = pd.DataFrame(all_matched_rows)\n",
    "        \n",
    "        # Create a filename for this service program\n",
    "        safe_filename = f\"service_program_{index}_matches.csv\"\n",
    "        \n",
    "        # Save to CSV\n",
    "        matches_df.to_csv(os.path.join(output_dir, safe_filename), index=False)\n",
    "    \n",
    "    # Print progress\n",
    "    if index % 10 == 0:\n",
    "        print(f\"Processed {index} rows\")\n",
    "\n",
    "print(f\"Completed! All match files saved to {output_dir} directory.\")\n",
    "\n",
    "# Display summary of what was created\n",
    "print(f\"Created CSV files with matching posts:\")\n",
    "for i, filename in enumerate(sorted(os.listdir(output_dir))):\n",
    "    if i < 10:  # Show first 10 examples\n",
    "        try:\n",
    "            match_df = pd.read_csv(os.path.join(output_dir, filename))\n",
    "            print(f\"- {filename}: {len(match_df)} matched original posts\")\n",
    "        except:\n",
    "            print(f\"- {filename}: Could not read file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_langchain_chroma._cosine_relevance_score_fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
