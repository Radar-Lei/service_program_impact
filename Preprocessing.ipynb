{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 筛选跟每个service program 相关的posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_evaluation(directory, output_directory):\n",
    "#     # 创建输出目录（如果不存在）\n",
    "#     Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # 选择设备\n",
    "#     if torch.cuda.is_available():\n",
    "#         device = \"cuda:1\"\n",
    "#     elif torch.backends.mps.is_available():\n",
    "#         device = \"mps\"\n",
    "#     else:\n",
    "#         device = \"cpu\"\n",
    "    \n",
    "#     print(f\"设备: {device}\")\n",
    "    \n",
    "#     # 加载多语言情感分析模型\n",
    "#     model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n",
    "#     print(f\"\\n > 正在加载模型 '{model_name}' \")\n",
    "    \n",
    "#     hf_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#     hf_model.to(device)\n",
    "#     hf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "#     # 为深圳地铁站定义关键词过滤\n",
    "#     sz_metro_stations = [\n",
    "#         \"机场东\", \"机场\", \"宝安中心\", \"前海湾\", \"鲤鱼门\", \"大新\", \"桃园\", \"深大\", \"科苑\", \"白石洲\", \n",
    "#         \"世界之窗\", \"华侨城\", \"侨城北\", \"香蜜湖\", \"车公庙\", \"竹子林\", \"招商银行大厦\", \"深康\", \"黄贝岭\", \n",
    "#         \"黄贝\", \"新秀\", \"莲塘\", \"梧桐山南\", \"梧桐山\", \"盐田路\", \"沙头角\", \"海山\", \"罗湖\", \"国贸\", \n",
    "#         \"老街\", \"大剧院\", \"科学馆\", \"华强路\", \"岗厦\", \"会展中心\", \"香蜜\", \"深南香蜜\", \"红树湾\", \"后海\", \n",
    "#         \"南山\", \"科技园\", \"大学城\", \"桃源村\", \"龙珠\", \"龙华\", \"清湖\", \"碧海湾\", \"铁路公园\", \"西丽湖\"\n",
    "#     ]\n",
    "    \n",
    "#     # 定义关键词用于过滤\n",
    "#     keywords = [\"深圳\", \"深铁\"] + sz_metro_stations\n",
    "    \n",
    "#     # 处理目录中的所有CSV文件\n",
    "#     for csv_file in Path(directory).glob('*.csv'):\n",
    "#         print(f\"\\n正在处理文件: {csv_file} 时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "#         # 使用pandas读取CSV\n",
    "#         df = pd.read_csv(csv_file)\n",
    "#         print(f\"{csv_file.name}中的总行数: {len(df)}\")\n",
    "        \n",
    "#         # 检查\"微博正文\"列是否存在\n",
    "#         if \"微博正文\" not in df.columns:\n",
    "#             print(f\"警告: '{csv_file}'中未找到'微博正文'列，跳过该文件。\")\n",
    "#             continue\n",
    "        \n",
    "#         # 过滤包含深圳地铁相关关键词的行\n",
    "#         filtered_df = df[df[\"微博正文\"].apply(lambda text: \n",
    "#             any(keyword in str(text) for keyword in keywords) if pd.notna(text) else False\n",
    "#         )]\n",
    "        \n",
    "#         print(f\"过滤后的行数: {len(filtered_df)}\")\n",
    "        \n",
    "#         store_posts = []\n",
    "        \n",
    "#         # 对每行进行情感分析\n",
    "#         for index, row in filtered_df.iterrows():\n",
    "#             if index % 100 == 0:\n",
    "#                 print(f\"已处理 {index} 行，时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                \n",
    "#             each_post = row['微博正文']\n",
    "            \n",
    "#             # 跳过过长的文本\n",
    "#             if len(str(each_post)) > 512:\n",
    "#                 continue\n",
    "                \n",
    "#             # 使用模型进行情感分析\n",
    "#             inputs = hf_tokenizer(str(each_post), return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 logits = hf_model(**inputs).logits\n",
    "            \n",
    "#             logits = logits.cpu().numpy()[0]\n",
    "            \n",
    "#             # 使用sigmoid函数归一化\n",
    "#             normalized = 1 / (1 + np.exp(-logits))\n",
    "            \n",
    "#             # 只保存明显的积极或消极情感\n",
    "#             if not (normalized[0] == max(normalized) or normalized[2] == max(normalized) and abs(max(normalized) - normalized[1]) > 0.1):\n",
    "#                 continue\n",
    "                \n",
    "#             # 将行转换为字典并添加情感分数\n",
    "#             post_dict = row.to_dict()\n",
    "#             post_dict['Negative'] = float(normalized[0])\n",
    "#             post_dict['Neutral'] = float(normalized[1])\n",
    "#             post_dict['Positive'] = float(normalized[2])\n",
    "#             store_posts.append(post_dict)\n",
    "        \n",
    "#         # 将结果保存为CSV\n",
    "#         if store_posts:\n",
    "#             output_df = pd.DataFrame(store_posts)\n",
    "#             output_filename = f\"senti_{csv_file.name}\"\n",
    "#             output_path = os.path.join(output_directory, output_filename)\n",
    "#             output_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "#             print(f\"已保存情感分析结果到: {output_path}\")\n",
    "#         else:\n",
    "#             print(f\"警告: 文件 {csv_file.name} 没有符合条件的内容\")\n",
    "\n",
    "\n",
    "# # 设置路径\n",
    "# directory_path = '深圳地铁'  # 输入目录\n",
    "# output_directory = '深圳地铁_情感分析'  # 输出目录\n",
    "\n",
    "# start_time = datetime.datetime.now()\n",
    "# print(f\"情感分析开始时间: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# sentiment_evaluation(directory_path, output_directory)\n",
    "\n",
    "# end_time = datetime.datetime.now()\n",
    "# print(f\"情感分析结束时间: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "# time_used = (end_time - start_time).total_seconds() / 60\n",
    "# print(f\"情感分析用时: {time_used:.2f} 分钟\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从已经过滤后的数据匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path\n",
    "directory_path = 'senti_results_SZ'\n",
    "all_docs = []\n",
    "\n",
    "# Define the Shenzhen metro station names\n",
    "sz_metro_stations = [\n",
    "    \"机场东\", \"机场\", \"宝安中心\", \"前海湾\", \"鲤鱼门\", \"大新\", \"桃园\", \"深大\", \"科苑\", \"白石洲\", \n",
    "    \"世界之窗\", \"华侨城\", \"侨城北\", \"香蜜湖\", \"车公庙\", \"竹子林\", \"招商银行大厦\", \"深康\", \"黄贝岭\", \n",
    "    \"黄贝\", \"新秀\", \"莲塘\", \"梧桐山南\", \"梧桐山\", \"盐田路\", \"沙头角\", \"海山\", \"罗湖\", \"国贸\", \n",
    "    \"老街\", \"大剧院\", \"科学馆\", \"华强路\", \"岗厦\", \"会展中心\", \"香蜜\", \"深南香蜜\", \"红树湾\", \"后海\", \n",
    "    \"南山\", \"科技园\", \"大学城\", \"桃源村\", \"龙珠\", \"龙华\", \"清湖\", \"碧海湾\", \"铁路公园\", \"西丽湖\"\n",
    "]\n",
    "\n",
    "# Define keywords for filtering\n",
    "keywords = [\"深圳\", \"深铁\"] + sz_metro_stations\n",
    "\n",
    "# Iterate through CSV files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Use pandas to read the CSV and preserve row information\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if \"微博正文\" column exists\n",
    "        if \"微博正文\" not in df.columns:\n",
    "            print(f\"Warning: '微博正文' column not found in {file_path}. Skipping file.\")\n",
    "            continue\n",
    "        \n",
    "        # Filter rows where \"微博正文\" contains any of the keywords\n",
    "        filtered_df = df[df[\"微博正文\"].apply(lambda text: \n",
    "            any(keyword in str(text) for keyword in keywords) if pd.notna(text) else False\n",
    "        )]\n",
    "        \n",
    "        # Convert each filtered row to a Document with appropriate metadata\n",
    "        for index, row in filtered_df.iterrows():\n",
    "            # Convert row to string representation\n",
    "            content = row[\"微博正文\"]\n",
    "            # Create metadata with file source and row index\n",
    "            metadata = {\n",
    "                'source': file_path,\n",
    "                'row': index,  # Keep original index from source file\n",
    "                # You can add other metadata from your CSV if needed\n",
    "            }\n",
    "            doc = Document(page_content=content, metadata=metadata)\n",
    "            all_docs.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} documents (rows) from CSVs after filtering for Shenzhen metro related content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "texts_all = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_directory = 'docs/chroma_rag/'\n",
    "# posts_langchain_chroma = Chroma.from_documents(\n",
    "#     documents=texts_all,\n",
    "#     collection_name=\"SZ_posts_senti\",\n",
    "#     embedding=hg_embeddings,\n",
    "#     persist_directory=persist_directory,\n",
    "#     collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/wdtcvnz52_1ffx0skvy0cyqm0000gn/T/ipykernel_30642/1355148894.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  posts_langchain_chroma = Chroma(\n"
     ]
    }
   ],
   "source": [
    "persist_directory = 'docs/chroma_rag/'\n",
    "posts_langchain_chroma = Chroma(\n",
    "    collection_name=\"SZ_posts_senti\",\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=hg_embeddings,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"} # this make sure the similarity socre between [0,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description Data for Service Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"service_program_data/SPD_SZ_zh.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('senti_results_SZ/senti_cleaned_深圳 地铁 201901 1.0.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows\n",
      "Processed 10 rows\n",
      "Processed 20 rows\n",
      "Completed! All match files saved to similarity_threshold=0.4 directory.\n",
      "- service_program_0_matches.csv: 965 matched original posts\n",
      "- service_program_10_matches.csv: 10000 matched original posts\n",
      "- service_program_11_matches.csv: 10000 matched original posts\n",
      "- service_program_12_matches.csv: 1089 matched original posts\n",
      "- service_program_13_matches.csv: 10000 matched original posts\n",
      "- service_program_14_matches.csv: 4153 matched original posts\n",
      "- service_program_15_matches.csv: 995 matched original posts\n",
      "- service_program_16_matches.csv: 10000 matched original posts\n",
      "- service_program_17_matches.csv: 248 matched original posts\n",
      "- service_program_18_matches.csv: 10000 matched original posts\n",
      "- service_program_19_matches.csv: 1218 matched original posts\n",
      "- service_program_1_matches.csv: 87 matched original posts\n",
      "- service_program_20_matches.csv: 2547 matched original posts\n",
      "- service_program_21_matches.csv: 10000 matched original posts\n",
      "- service_program_22_matches.csv: 3336 matched original posts\n",
      "- service_program_2_matches.csv: 1849 matched original posts\n",
      "- service_program_3_matches.csv: 105 matched original posts\n",
      "- service_program_4_matches.csv: 866 matched original posts\n",
      "- service_program_5_matches.csv: 473 matched original posts\n",
      "- service_program_6_matches.csv: 10000 matched original posts\n",
      "- service_program_7_matches.csv: 57 matched original posts\n",
      "- service_program_8_matches.csv: 325 matched original posts\n",
      "- service_program_9_matches.csv: 245 matched original posts\n",
      "\n",
      "Summary saved to similarity_threshold=0.4/0.matched_summary.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Create directory for output if it doesn't exist\n",
    "similarity_threshold = 0.45\n",
    "\n",
    "output_dir = f'similarity_threshold={similarity_threshold}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the service program CSV file\n",
    "service_program_df = pd.read_csv('service_program_data/SPD_SZ_zh.csv')\n",
    "\n",
    "# Create a cache for the source dataframes to avoid reloading them\n",
    "source_df_cache = {}\n",
    "\n",
    "# Iterate through each row of the CSV\n",
    "for index, row in service_program_df.iterrows():\n",
    "    # Concatenate the first three columns as service_dimension\n",
    "    service_dimension = ' '.join([str(row[col]) for col in service_program_df.columns[1:3] \n",
    "                            if pd.notna(row[col]) and str(row[col]) != 'nan'])\n",
    "    \n",
    "    # Search for similar documents in the vector database\n",
    "    docs = posts_langchain_chroma.similarity_search_with_relevance_scores(service_dimension, k=10000, score_threshold=similarity_threshold)\n",
    "    # Create a dictionary to group matched rows by source file\n",
    "    matches_by_source = {}\n",
    "    \n",
    "    # Process each matched document\n",
    "    # each doc is a tuple\n",
    "    for doc in docs:\n",
    "        source = doc[0].metadata.get('source', None)\n",
    "        row_index = doc[0].metadata.get('row', None)\n",
    "        \n",
    "        if source and row_index is not None:\n",
    "            # Convert row to integer if it's a numeric string\n",
    "            try:\n",
    "                row_index = int(row_index)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "            if source not in matches_by_source:\n",
    "                matches_by_source[source] = []\n",
    "            \n",
    "            matches_by_source[source].append(row_index)\n",
    "    \n",
    "    # Create a list to store all matching rows from original sources\n",
    "    all_matched_rows = []\n",
    "    \n",
    "    # For each source file, get the matching rows\n",
    "    for source, row_indices in matches_by_source.items():\n",
    "        # Load the source file if not in cache\n",
    "        if source not in source_df_cache:\n",
    "            try:\n",
    "                source_df_cache[source] = pd.read_csv(source)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {source}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        source_df = source_df_cache[source]\n",
    "        \n",
    "        # Get rows from original source file\n",
    "        for row_idx in row_indices:\n",
    "            try:\n",
    "                if 0 <= row_idx < len(source_df):\n",
    "                    row_data = source_df.iloc[row_idx].to_dict()\n",
    "                    row_data['original_source'] = source\n",
    "                    row_data['original_row'] = row_idx\n",
    "                    all_matched_rows.append(row_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing row {row_idx} in {source}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if all_matched_rows:\n",
    "        matches_df = pd.DataFrame(all_matched_rows)\n",
    "        \n",
    "        # Create a filename for this service program\n",
    "        safe_filename = f\"service_program_{index}_matches.csv\"\n",
    "        \n",
    "        # Save to CSV\n",
    "        matches_df.to_csv(os.path.join(output_dir, safe_filename), index=False)\n",
    "    \n",
    "    # Print progress\n",
    "    if index % 10 == 0:\n",
    "        print(f\"Processed {index} rows\")\n",
    "\n",
    "print(f\"Completed! All match files saved to {output_dir} directory.\")\n",
    "\n",
    "# 创建摘要文件\n",
    "summary_file_path = os.path.join(output_dir, '0.matched_summary.txt')\n",
    "with open(summary_file_path, 'w', encoding='utf-8') as summary_file:\n",
    "    summary_file.write(f\"Created CSV files with matching posts:\\n\")\n",
    "    for i, filename in enumerate(sorted(os.listdir(output_dir))):\n",
    "        if filename == '0.matched_summary.txt':  # 跳过摘要文件本身\n",
    "            continue\n",
    "        try:\n",
    "            match_df = pd.read_csv(os.path.join(output_dir, filename))\n",
    "            summary_line = f\"- {filename}: {len(match_df)} matched original posts\\n\"\n",
    "            summary_file.write(summary_line)\n",
    "            print(summary_line, end='')\n",
    "        except:\n",
    "            summary_line = f\"- {filename}: Could not read file\\n\"\n",
    "            summary_file.write(summary_line)\n",
    "            print(summary_line, end='')\n",
    "\n",
    "print(f\"\\nSummary saved to {summary_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
